{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minyoy/ML-DL-Basics-study/blob/main/wk5/basic_wk5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpwpcAMdgVnC"
      },
      "source": [
        "# **5주차: Multi-layer Perceptron (1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-m1YIiDonGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505b69a7-fe4e-4d6c-db80-f8ff7d00c244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# install package\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uoYgORDlKfS"
      },
      "source": [
        "## **퍼셉트론 (Perceptron)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlGH7IPbkdum"
      },
      "source": [
        "#### **퍼셉트론이란?**\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dn/baZLNt/btqMuOFdSgZ/UmP7XA7Y2fNroun6fFjKxk/img.png\" width=\"350\">\n",
        "\n",
        "생물학적인 신경계의 기본 단위인 뉴런의 동작 과정을 수학적으로 비슷하게 모델링한 것으로, 다수의 입력으로부터 하나의 결과를 내보내는 초기 형태의 인공신경망\n",
        "\n",
        "$ y = \\sigma(Wx+b) $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$x$: 입력값(Input) &#160;&#160; $W$: 가중치(Weight) &#160;&#160; $b$: 편향(Bias) &#160;&#160; $\\sigma$: 활성화 함수(Activation function)\n",
        "\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pvi3rpggVnF"
      },
      "source": [
        "#### **단층 퍼셉트론의 한계**\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images/ksolar03/post/291fbccb-9433-422d-ab73-c0a42dc3ca44/image.png\" width=\"500\">\n",
        "\n",
        "AND, OR와 같은 선형 분류 문제는 해결할 수 있지만, XOR 문제와 같은 비선형 분류 문제는 해결할 수 없음 </br>\n",
        "\n",
        "따라서, 비선형 분류 문제를 해결하기 위해서는 퍼셉트론에 비선형 요소가 필요하며, 다층 퍼셉트론을 이용하여 비선형 분류 문제를 해결할 수 있음\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images%2Fskyepodium%2Fpost%2F63930671-cb82-43a2-825b-e822beec415f%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-05-19%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.03.37.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXSSRe5IgVnF"
      },
      "source": [
        "**Perceptron 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ystoutgVnF",
        "outputId": "13e163c8-65de-49f8-dbc1-fdacd0d990c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2841]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "num_features = 5\n",
        "perceptron = nn.Sequential(nn.Linear(num_features, 1), nn.Sigmoid())\n",
        "\n",
        "data = torch.randn(1,5)\n",
        "output = perceptron(data)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YfjegHh79d"
      },
      "source": [
        "## **다층 퍼셉트론 (Multi-layer Perceptron, MLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lo6kNPYG9xs"
      },
      "source": [
        "<img src=\"https://ifh.cc/g/TGRpSs.jpg\" width=\"450\">\n",
        "\n",
        "인간의 뇌를 모방하여 만든 네트워크로, 퍼셉트론이 여러겹으로 쌓여있는 형태 </br>\n",
        "\n",
        "각각의 퍼셉트론은 노드(Node) 또는 유닛(Unit)이라고 부르고, 노드들 간의 연결은 가중치로 표현 </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzuQN_rggVnF"
      },
      "source": [
        "#### **입력층 (Input Layer)**\n",
        "\n",
        "데이터가 입력되는 층으로, 데이터의 특성 수 만큼의 노드를 가짐\n",
        "\n",
        "어떠한 계산도 수행하지 않고, 다음 층으로 값을 그대로 전달"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aS3W9BtgVnG"
      },
      "source": [
        "#### **은닉층 (Hidden Layer)**\n",
        "\n",
        "입력층과 출력층 사이에 있는 층으로, 노드의 수는 자유롭게 구성할 수 있음\n",
        "\n",
        "입력 신호가 가중치와 편향과 함께 계산되고 비선형 함수인 활성화 함수를 지남\n",
        "\n",
        "일반적으로 2개 이상의 은닉층을 가지고 있으면 심층신경망(Deep Neural Network, DNN)이라고 부름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auWFwx2tgVnG"
      },
      "source": [
        "#### **출력층 (Output Layer)**\n",
        "\n",
        "연산을 마친 결과가 출력되는 층\n",
        "\n",
        "풀어야 할 문제에 따라 노드의 수와 활성화 함수가 달라짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMis41pfgVnG"
      },
      "source": [
        "#### **다층 퍼셉트론의 특징**\n",
        "\n",
        "<img src=\"https://ifh.cc/g/rdH9Jj.png\" width=\"258\">\n",
        "<img src=\"https://ifh.cc/g/sPSWS9.png\" width=\"450\">\n",
        "\n",
        "입력 데이터가 여러 은닉층에서 비선형 (Non-linearity) 변환을 거치면서 새로운 특징을 학습하며 (Feature Learning) </br>\n",
        "\n",
        "이를 통해 비선형 분류 문제를 해결할 수 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPjj8yiPgVnG"
      },
      "source": [
        "**Multi-layer Perceptron 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK22N-DHgVnG",
        "outputId": "a42ed320-87a2-43b3-8ceb-34ea52cefd59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4331]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "num_features = 5\n",
        "h_dim = 3\n",
        "mlp = nn.Sequential(nn.Linear(num_features, h_dim), nn.Sigmoid(),\n",
        "                    nn.Linear(h_dim, h_dim), nn.Sigmoid(),\n",
        "                    nn.Linear(h_dim, 1), nn.Sigmoid())\n",
        "\n",
        "data = torch.randn(1,5)\n",
        "output = mlp(data)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuaRcEX3gVnG"
      },
      "source": [
        "## **딥러닝 (Deep Learning)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP7--5gjgVnG"
      },
      "source": [
        "<img src=\"https://ifh.cc/g/RvYW9L.jpg\" width=\"500\">\n",
        "\n",
        "심층신경망을 학습시키는 것을 딥러닝이라고 함 </br>\n",
        "\n",
        "딥러닝을 진행하기 위해서는 모델, 데이터, 손실함수, 옵티마이저가 필요함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asSPSOeggVnG"
      },
      "source": [
        "### **데이터 (Data)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVUoTPatgVnG"
      },
      "source": [
        "<img src=\"https://blog.kakaocdn.net/dn/bQYQPK/btrHiSNYyo9/fkgBAcVKaIx3ooknZ5zXl0/img.png\" width=\"450\">\n",
        "\n",
        "데이터 표현을 위한 기본 구조로 텐서(Tensor)를 사용하며, Numpy 패키지의 ndarray 객체와 유사함 </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BPa5pNTgVnG"
      },
      "source": [
        "**Tensor 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh9iTezLgVnH",
        "outputId": "647efcbb-cdef-48df-faa1-aea5449353a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor: tensor([0, 1, 2])\n",
            "tensor type: torch.int64\n",
            "tensor shape: torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.tensor([0,1,2])\n",
        "\n",
        "print(f\"tensor: {tensor}\")\n",
        "print(f\"tensor type: {tensor.dtype}\")\n",
        "print(f\"tensor shape: {tensor.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai2hWfe4gVnH"
      },
      "source": [
        "### **모델 (Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvYXlQwrgVnH"
      },
      "source": [
        "모델은 각 층과 활성화 함수로 구성되어 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNTqREdXgVnH"
      },
      "source": [
        "#### **층 (Layer)**\n",
        "\n",
        "신경망의 핵심 데이터 구조로, 하나 이상의 텐서를 입력받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈 </br>\n",
        "\n",
        "각 층별로 가중치 유무가 다르며 가중치는 역전파와 옵티마이저에 의해 학습되는 하나 이상의 텐서를 의미함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiYlEcLgVnH"
      },
      "source": [
        "##### **완전연결계층 (Fully-connected Layer)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/1*VHOUViL8dHGfvxCsswPv-Q.png\" width=\"300\">\n",
        "\n",
        "모든 노드가 서로 연결되어 있는 층으로, 밀집층 (Dense Layer)이라고도 부름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzkiY8XogVnH"
      },
      "source": [
        "**Fully-connected Layer 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8_gKgHcgVnH",
        "outputId": "d530ebaf-f3c6-4ca0-e3c9-ffa2a52281e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[-1.8058,  1.1198, -0.2731,  0.0821, -0.6618]])\n",
            "input shape: torch.Size([1, 5])\n",
            "\n",
            "output: tensor([[ 0.6438, -0.9971,  0.2107]], grad_fn=<AddmmBackward0>)\n",
            "output shape: torch.Size([1, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "h1_dim = 5\n",
        "h2_dim = 3\n",
        "x = torch.randn(1,5) # 정규분포(평균 0, 표준편차 1)를 따르는 난수를 생성하는 함수로 입력값은 생성할 텐서의 shape(크기)를 의미\n",
        "\n",
        "fc = nn.Linear(h1_dim, h2_dim)\n",
        "output = fc(x)\n",
        "\n",
        "print(f\"input: {x}\")\n",
        "print(f\"input shape: {x.size()}\\n\")\n",
        "\n",
        "print(f\"output: {output}\")\n",
        "print(f\"output shape: {output.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f4Fi_FQgVnH"
      },
      "source": [
        "##### **평탄화계층 (Flatten Layer)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*_Cb1dzBhciwRi9y1-J6qjQ.png\" width=\"450\">\n",
        "\n",
        "배치 크기(또는 데이터 크기)를 제외하고 데이터를 1차원으로 쭉 펼치는 층으로, 가중치는 따로 없음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qvBFWVmgVnH"
      },
      "source": [
        "**Flatten Layer 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIKfM-o_gVnH",
        "outputId": "1d760819-5051-4741-b41b-efa60da64ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[[ 1.7425, -0.9942, -0.7627],\n",
            "         [-0.9275, -0.1685,  0.2359],\n",
            "         [ 0.2866,  0.7387,  2.2588]],\n",
            "\n",
            "        [[-0.4658,  1.7903,  0.1089],\n",
            "         [-0.7114, -1.1871,  1.1720],\n",
            "         [ 0.2332,  1.2128, -0.3460]]])\n",
            "input shape: torch.Size([2, 3, 3])\n",
            "\n",
            "output: tensor([[ 1.7425, -0.9942, -0.7627, -0.9275, -0.1685,  0.2359,  0.2866,  0.7387,\n",
            "          2.2588],\n",
            "        [-0.4658,  1.7903,  0.1089, -0.7114, -1.1871,  1.1720,  0.2332,  1.2128,\n",
            "         -0.3460]])\n",
            "output shape: torch.Size([2, 9])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.randn(2,3,3) # 3×3 행렬이 2개 들어있는 구조\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "output = flatten(x)\n",
        "\n",
        "print(f\"input: {x}\")\n",
        "print(f\"input shape: {x.size()}\\n\")\n",
        "\n",
        "print(f\"output: {output}\")\n",
        "print(f\"output shape: {output.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-HKZpBbgVnI"
      },
      "source": [
        "이외에도 다양한 층이 존재\n",
        "\n",
        "PyTorch : https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDjuFnY8gVnI"
      },
      "source": [
        "#### **활성화 함수 (Activation Function)**\n",
        "\n",
        "<img src=\"https://hwk0702.github.io/img/activate1.png\" width=\"450\">\n",
        "\n",
        "은닉층과 출력층 결과값에 사용되는 함수로, 비선형 함수를 사용 </br>\n",
        "\n",
        "대표적으로 Sigmoid, tanh, ReLU, Leaky ReLU 등의 활성화 함수가 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0NVWvFfgVnI"
      },
      "source": [
        "##### **활성화 함수로 선형 함수를 사용한다면?**\n",
        "\n",
        "$ (W_{1} W_{2} W_{3})x = Wx $\n",
        "\n",
        "여러 선형 함수의 조합은 하나의 선형 함수로 표현할 수 있음\n",
        "\n",
        "따라서, 활성화 함수로 선형 함수를 사용하게 되면 단층 퍼셉트론과 다름 없게 되고 비선형 변환을 수행하지 못함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEsxLwCigVnI"
      },
      "source": [
        "##### **Sigmoid 함수**\n",
        "\n",
        "$ f(x) = \\dfrac{1}{1+e^{-x}} $\n",
        "\n",
        "시그모이드 함수는 (0,1)의 출력 범위를 가지는 함수 </br> </br>\n",
        "\n",
        "<img src=\"https://ifh.cc/g/wsRMTR.jpg\" width=\"500\">\n",
        "\n",
        "역전파를 시킬 때 기울기 소실 (Gradient Vanishing) 문제가 발생하여 모델의 파라미터 학습이 제대로 이루어지지 않기 때문에 일반적으로 은닉층에서는 잘 쓰이지 않으며,\n",
        "\n",
        "이진 분류 문제에서, 확률값을 표현하기 위한 함수로 출력층에서 쓰이는 경우가 종종 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul5yTuIZgVnI"
      },
      "source": [
        "##### **tanh 함수**\n",
        "\n",
        "$ f(x) = \\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $\n",
        "\n",
        "tanh 함수는 (-1,1)의 출력 범위를 가지는 함수 </br>\n",
        "\n",
        "시그모이드 함수와 마찬가지로, 역전파를 시킬 때 기울기 소실 문제가 발생하기 때문에 일반적으로 은닉층에서는 잘 쓰이지 않음 </br> </br>\n",
        "\n",
        "<img src=\"https://curt-park.github.io/images/lstm_strong_on_gradient_vanishing/RNN.png\" width=\"300\">\n",
        "\n",
        "하지만 시그모이드 함수와는 다르게 zero-center의 특징을 가지고 있기 때문에 더 좋은 성능을 발휘하며, </br>\n",
        "\n",
        "RNN 계열의 모델에서는 출력값이 발산하지 않는 것이 중요하기 때문에, RNN 계열 모델의 은닉층에서는 tanh 함수를 주로 사용함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wep58PFSgVnI"
      },
      "source": [
        "##### **ReLU 함수**\n",
        "\n",
        "$ f(x) = max(0, x) $\n",
        "\n",
        "ReLU 함수는 [0, $\\infty$)의 출력 범위를 가지는 함수 </br>\n",
        "\n",
        "양수인 입력값에 대해서는 1의 기울기를 가지기 때문에 기울기 소실 문제가 일어나지 않으며, </br>\n",
        "\n",
        "간단하면서도 연산량이 적고 안정적으로 학습을 가능하게 하기 때문에 일반적으로 은닉층에서 많이 쓰이는 활성화 함수 </br> </br>\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:546/0*V4s_iyhAJxd2FaaB.png\" width=\"400\">\n",
        "\n",
        "하지만, 음수인 입력값에 대해서는 모두 0으로 변환되기 때문에 해당 정보를 이용하지 못하는 단점이 있는데, </br>\n",
        "\n",
        "이러한 단점을 보완한 함수로 Leaky ReLU 함수가 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2VR9ZkMgVnI"
      },
      "source": [
        "**ReLU 함수 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYkRwKSDgVnJ",
        "outputId": "4b69afd9-bdb2-4c92-dfe6-6ee4589cf376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[-0.0120,  0.7495,  0.6459, -1.5374,  0.3889]])\n",
            "input shape: torch.Size([1, 5])\n",
            "\n",
            "output: tensor([[0.0000, 0.7495, 0.6459, 0.0000, 0.3889]])\n",
            "output shape: torch.Size([1, 5])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.randn(1,5)\n",
        "\n",
        "relu = nn.ReLU()\n",
        "output = relu(x)\n",
        "\n",
        "print(f\"input: {x}\")\n",
        "print(f\"input shape: {x.size()}\\n\")\n",
        "\n",
        "print(f\"output: {output}\")\n",
        "print(f\"output shape: {output.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHWWaQefgVnJ"
      },
      "source": [
        "##### **Softmax 함수**\n",
        "\n",
        "$ f(x_{i}) = \\dfrac{e^{-x_{i}}}{\\Sigma_{j=1}^{N} e^{-x_{j}}} $\n",
        "\n",
        "소프트맥스 함수는 (0,1)의 출력 범위를 가지는 함수 </br> </br>\n",
        "\n",
        "<img src=\"https://vitalflux.com/wp-content/uploads/2020/10/Softmax-Function.png\" width=\"400\">\n",
        "\n",
        "모든 출력값의 합이 1이고, 입력값이 큰 순서대로 출력값도 크게 나오기 때문에 </br>\n",
        "\n",
        "다중 분류 문제에서 각 클래스에 대한 확률값을 표현하기 위한 함수로, 출력층에서 주로 사용함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGu9z3kZgVnJ"
      },
      "source": [
        "**Softmax 함수 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esF10ifkgVnJ",
        "outputId": "414d1454-307c-47d6-f083-1943bf974f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([[0.6740, 1.4890, 0.6908, 0.4289, 1.1718]])\n",
            "input shape: torch.Size([1, 5])\n",
            "\n",
            "output: tensor([[0.1492, 0.3370, 0.1517, 0.1167, 0.2454]])\n",
            "output shape: torch.Size([1, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "x = torch.randn(1,5)\n",
        "\n",
        "softmax = nn.Softmax()\n",
        "output = softmax(x)\n",
        "\n",
        "print(f\"input: {x}\")\n",
        "print(f\"input shape: {x.size()}\\n\")\n",
        "\n",
        "print(f\"output: {output}\")\n",
        "print(f\"output shape: {output.size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJx6kVoJgVnJ"
      },
      "source": [
        "현재는 GELU, SiLU 등의 활성화 함수도 많이 사용하며, 이외에도 다양한 활성화 함수가 존재\n",
        "\n",
        "PyTorch : https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPFXpI3SgVnJ"
      },
      "source": [
        "### **손실함수 (Loss Function)**\n",
        "\n",
        "손실함수는 입력 데이터의 실제값과 예측값의 차이를 측정하는 함수로, </br>\n",
        "\n",
        "모델의 파라미터는 손실함수의 값이 최소가 되는 방향으로 학습됨 </br>\n",
        "\n",
        "역전파를 위해서 손실함수는 미분 가능한 함수를 사용해야 하며, 해결해야 하는 문제에 따라 사용되는 손실함수도 다름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Y-irqPgVnJ"
      },
      "source": [
        "#### **회귀 (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrGckAIcgVnK"
      },
      "source": [
        "##### **평균 절대오차 (Mean Absolute Error, MAE)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1152/1*8BQhdKu1nk-tAAbOR17qGg.png\" width=\"350\">\n",
        "\n",
        "$ L = \\frac{1}{N}\\sum_{i=1}^N \\left | y_i - \\tilde{y}_i \\right |$\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$y_i$ : $i\\ $번째 학습 데이터의 정답 </br>\n",
        "$\\tilde{y}_i$ : $i\\ $번째 학습 데이터에 대한 모델의 예측값 </br>\n",
        "$N$ : 전체 데이터의 개수 </br> </br>\n",
        "\n",
        "</span>\n",
        "\n",
        "정답과 예측한 값의 차이가 커져도 오차는 일정하게 증가하는 특징을 가지고 있기 때문에 이상치(Outlier)에 강함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtQUeY9ZgVnK"
      },
      "source": [
        "##### **평균 제곱오차 (Mean Squared Error, MSE)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1152/1*EqTaoCB1NmJnsRYEezSACA.png\" width=\"350\">\n",
        "\n",
        "$ L = \\frac{1}{N}\\sum_{i=1}^N ( y_i - \\tilde{y}_i)^2 $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$y_i$ : $i\\ $번째 학습 데이터의 정답 </br>\n",
        "$\\tilde{y}_i$ : $i\\ $번째 학습 데이터에 대한 모델의 예측값 </br>\n",
        "$N$ : 전체 데이터의 개수 </br> </br>\n",
        "\n",
        "</span>\n",
        "\n",
        "회귀에서 가장 많이 쓰이는 손실 함수 중 하나로, 정답과 예측한 값의 차이가 클수록 더 많은 페널티를 부여하기 때문에 모델의 파라미터가 더 빠르게 수렴함 </br>\n",
        "\n",
        "하지만, MAE 손실함수에 비해서는 이상치에 약한 특징을 지님"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FytZiDsgVnK"
      },
      "source": [
        "**MSE Loss 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYN4lwKlgVnK",
        "outputId": "8eda8fcc-6ee9-41fa-bb5a-0fe0ba50f22c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true value: tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "prediction value: tensor([[-0.5549],\n",
            "        [ 0.9619],\n",
            "        [ 0.0894],\n",
            "        [-0.5288],\n",
            "        [ 0.7511]])\n",
            "MSE loss: 1.1295175552368164\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "y_true = torch.ones((5,1)) # 모든 원소가 1인 텐서를 생성하는 함수\n",
        "y_pred = torch.randn(5,1)\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "loss = mse(y_pred, y_true)\n",
        "\n",
        "print(f\"true value: {y_true}\")\n",
        "print(f\"prediction value: {y_pred}\")\n",
        "print(f\"MSE loss: {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orVawjsTgVnK"
      },
      "source": [
        "##### **손실함수 MAE와 MSE 비교**\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*JTC4ReFwSeAt3kvTLq1YoA.png\" width=\"700\">\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp58DdS2gVnK"
      },
      "source": [
        "#### **분류 (Classification)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzTr67RHgVnK"
      },
      "source": [
        "##### **원-핫 인코딩 (One-Hot Encodeing)**\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1200/0*PO_ENSfL80nPRqIg\" width=\"650\">\n",
        "\n",
        "범주형(Categorical) 변수를 표현할 때 사용하며, 정답인 레이블을 제외하고 0으로 처리 </br>\n",
        "\n",
        "원-핫 인코딩으로 변환된 값들은 각 클래스에 대한 확률 분포라고 볼 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65niPNb6gVnK"
      },
      "source": [
        "##### **교차 엔트로피 오차 (CrossEntropy Loss)**\n",
        "\n",
        "<img src=\"https://wiki.cloudfactory.com/media/pages/docs/mp-wiki/loss/cross-entropy-loss/3e1efdec9f-1684131968/image-30.webp\" width=\"350\">\n",
        "\n",
        "$ L = - \\frac{1}{N}\\sum_{N} \\sum_{i}  y_i\\ log\\tilde{y}_i $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$y_i$ : 원-핫 인코딩의  $i\\ $번째 값 </br>\n",
        "$\\tilde{y}_i$ : 모델의 $i\\ $번째 출력값 </br>\n",
        "$N$ : 전체 데이터의 개수 </br> </br>\n",
        "\n",
        "</span>\n",
        "\n",
        "교차 엔트로피 오차는 두 확률 분포간의 차이를 측정하는 함수로,\n",
        "\n",
        "모델을 통해 예측된 확률 분포 (소프트맥스 함수를 적용한 모델의 출력값)와 실제 확률 분포 (원-핫 인코딩 값)을 비교함 </br> </br>\n",
        "\n",
        "$y_i$가 1인 상황에서 $\\tilde{y}_i$가 1에 가까울 수록 오차는 0에 가까워지고, 0에 가까울 수록 오차는 무한히 커지기 때문에\n",
        "\n",
        "모델의 파라미터가 빨리 수렴하는 특징을 지님"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRj5k8qgVnK"
      },
      "source": [
        "**CrossEnropy Loss 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24MAQX39gVnK",
        "outputId": "e5fc0ce5-5b2d-43f3-af69-78159a3d0485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true value (one-hot encoding): tensor([[0, 0, 1]])\n",
            "prediction value: tensor([[-2.2957, -0.3874,  0.0251]])\n",
            "CrossEntropy loss: 0.5654127597808838\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "y_true = torch.LongTensor([2])  # label encoding\n",
        "y_pred = torch.randn(1,3)\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = ce(y_pred, y_true)\n",
        "\n",
        "print(f\"true value (one-hot encoding): {F.one_hot(y_true, num_classes=3)}\") # 정수로 표현된 레이블을 one-hot 벡터로 바꿔주는 함수, num_classes는 전체 클래스 개수\n",
        "print(f\"prediction value: {y_pred}\")\n",
        "print(f\"CrossEntropy loss: {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGfdZbMgVnK"
      },
      "source": [
        "이외에도 다양한 손실함수가 존재함\n",
        "\n",
        "PyTorch : https://pytorch.org/docs/stable/nn.html#loss-functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzFk-bVlgVnK"
      },
      "source": [
        "### **역전파 (Backpropagation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRPX5b-bgVnK"
      },
      "source": [
        "#### **역전파 알고리즘**\n",
        "\n",
        "<img src=\"https://ifh.cc/g/D7qTn9.png\" width=\"350\">\n",
        "\n",
        "먼저, 데이터로 정방향 연산(Forward)을 통해 손실함수 값을 구함\n",
        "\n",
        "이후, 마지막 층부터 앞으로 하나씩 연쇄법칙(Chain Rule)을 이용하여 손실함수에 대한 학습 파라미터의 편미분값 ($\\frac{\\partial L}{\\partial W}$,$\\frac{\\partial L}{\\partial b}$)을 계산\n",
        "\n",
        "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*_KMMFvRP5X9kC59brI0ykw.png\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hRHcPy8gVnK"
      },
      "source": [
        "#### **역전파 알고리즘의 특징**\n",
        "\n",
        "손실함수 값을 한 번만 구하고, 행렬 연산과 연쇄법칙을 이용한 미분을 활용하기 때문에 학습 소요시간을 크게 단축할 수 있음\n",
        "\n",
        "하지만, 미분을 위한 중간값을 각 레이어별로 모두 저장하기 때문에 메모리를 많이 사용한다는 단점이 있음\n",
        "\n",
        "역전파로 계산된 편미분값은 결과 예측에 있어서 각 가중치 또는 편향이 얼마나 중요한지를 보여줄 수 있는 지표가 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThKReVbogVnK"
      },
      "source": [
        "**Backpropagation 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gd8VdY9gVnK",
        "outputId": "cb16fe56-fe7a-4c38-f76a-e10d74b09875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5465, -0.3807, -0.0164]], grad_fn=<MulBackward0>) tensor(-1.3145, grad_fn=<MeanBackward0>)\n",
            "gradient of x: tensor([[-0.1998,  0.1083, -0.0183]])\n",
            "gradient of y: tensor([[ 0.3039, -0.3906,  0.0999]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(1,3, requires_grad=True) # requires_grad는 자동미분 활성화, gradient가 추적됨\n",
        "y = torch.randn(1,3, requires_grad=True)\n",
        "\n",
        "z = x * y\n",
        "loss = torch.mean(z - torch.ones_like(z))\n",
        "# torch.ones_like(z)는 z와 같은 크기의 1로 채워진 텐서로, z-1 연산 후 평균을 내는 코드\n",
        "\n",
        "loss.backward()  # backpropagation\n",
        "\n",
        "print(f\"gradient of x: {x.grad}\")\n",
        "print(f\"gradient of y: {y.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9vYfPfUgVnK"
      },
      "source": [
        "다음의 링크에서 PyTorch 프레임워크의 AutoGrad 기능에 대해서 더 자세히 확인할 수 있음\n",
        "\n",
        "PyTorch : https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dHyB8F0gVnK"
      },
      "source": [
        "### **옵티마이저 (Optimizer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tolXnTW4vQIU"
      },
      "source": [
        "손실함수의 최소값을 찾는 일을 수행하는 알고리즘으로, 모델의 파라미터를 최적화시키는 역할을 담당함\n",
        "\n",
        "옵티마이저는 모델의 파라미터를 빠르고 안정적으로 수렴시켜야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anLxNvGBv1gW"
      },
      "source": [
        "#### **학습률 (Learning Rate)**\n",
        "\n",
        "모델의 파라미터를 업데이트 할 때, 한 번에 얼마만큼을 이동할지를 결정해주는 하이퍼파라미터\n",
        "\n",
        "학습률이 너무 크면 발산하거나 최저점에 도달하지 못하고, 너무 작으면 학습이 오래 걸리기 때문에 적절한 학습률을 지정해야 최저점에 잘 도달할 수 있음\n",
        "\n",
        "<br />\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/99D8634C5D89919B21\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HelzSXPyzdwI"
      },
      "source": [
        "#### **확률적 경사하강법 (Stochastic Gradient Descent, SGD)**\n",
        "\n",
        "$ \\theta_{t+1} = \\theta_{t} - \\mu \\frac{\\partial L}{\\partial \\theta_{t}} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$L$ : 손실함수 &#160;&#160; $\\theta$ : 모델의 파라미터 &#160;&#160; $\\mu$ : 학습률\n",
        "\n",
        "</span>\n",
        "</br>\n",
        "\n",
        "모든 데이터를 한번에 사용하지 않고 확률적으로 샘플을 하나씩 뽑아 모델의 파라미터를 업데이트하는 알고리즘으로,\n",
        "\n",
        "현재는 보통 SGD라고 하면 데이터를 미니배치 단위로 샘플링하여 모델을 학습시키는 미니배치 경사하강법 (Mini-batch SGD)을 의미함 </br> </br>\n",
        "\n",
        "모델의 파라미터를 한 번 업데이트 할 때 사용하는 데이터의 수가 적기 때문에,\n",
        "\n",
        "처리하는 속도가 빠르고 큰 메모리가 필요없어 매우 큰 데이터셋에 대해서도 학습이 가능함 </br> </br>\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb2KqoM%2FbtsBF19TTI6%2F5OsTFiG5HarqqxWNku3zTk%2Fimg.png\" width=\"400\">\n",
        "\n",
        "하지만, 샘플링되는 데이터에 따라 손실함수의 모양이 달라지기 때문에, 파라미터가 최적점까지 도달하는데 진동하면서 불안정하게 움직이며 느리게 수렴한다는 특징을 가짐\n",
        "\n",
        "또한, 기울기가 0인 지역 최소값(Local Minima)이나 안장점(Saddle Point)에 갇혀 전역 최소값(Global Minima)에 도달하지 못할 가능성이 높음 </br> </br>\n",
        "\n",
        "<img src=\"https://i.ibb.co/8NfMxWN/sgd-2.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYK0jM_vgVnL"
      },
      "source": [
        "**SGD 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1MXCGp_gVnL",
        "outputId": "d407d0ea-0e12-43f7-e61e-aabb48487431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameter before update: tensor([0., 0., 0., 0., 0.])\n",
            "parameter after update: tensor([-0.0065, -0.0050, -0.0059, -0.0107, -0.0241])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "param = nn.Parameter(torch.zeros(5)) # 0이 5개인 텐서 생성, 모델의 학습 가능한 파라미터를 등록해두는 곳으로 쓰임\n",
        "sgd = torch.optim.SGD([param], lr=1e-2) # 학습 대상 파라미터: [param], 학습률(learning rate): 0.01\n",
        "\n",
        "prev_param = param.clone() # 업데이트 전에 param 값을 복사해서 저장해두는 것.\n",
        "param.grad = torch.randn_like(param.data)  # gradient accumulation\n",
        "sgd.step()  # parameter update\n",
        "\n",
        "print(f\"parameter before update: {prev_param.data}\")\n",
        "print(f\"parameter after update: {param.data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mJ_Ju1rzker"
      },
      "source": [
        "#### **모멘텀 (Momentum)**\n",
        "\n",
        "$ v_{t} = \\alpha v_{t-1} + \\frac{\\partial L}{\\partial \\theta_{t}} $ </br>\n",
        "$ \\theta_{t+1} = \\theta_{t} - \\mu v_{t} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$L$ : 손실함수 &#160;&#160; $\\theta$ : 모델의 파라미터 &#160;&#160; $\\mu$ : 학습률 &#160;&#160; $\\alpha$ : 모멘텀 계수\n",
        "\n",
        "</span>\n",
        "</br>\n",
        "\n",
        "모델의 파라미터를 업데이트 하는데 현재 기울기뿐만이 아니라 이전의 기울기 정보를 활용하여 업데이트 하는 알고리즘\n",
        "\n",
        "관성처럼 이전 기울기의 속도를 유지하려고 하고, 모멘텀 계수를 통해서 이전 기울기의 속도를 얼마나 유지할지 결정할 수 있음 </br> </br>\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images/soraemon/post/d676caaa-ce0f-42c0-9f2b-1e0c5865d6ca/image.png\" width=\"300\">\n",
        "\n",
        "모델의 파라미터가 업데이트 되는데 현재 기울기에 이전 기울기의 일정 부분을 더하기 때문에,\n",
        "\n",
        "경사하강법에 비해서 더 긴 거리를 이동하여 더 빠르게 수렴하고, 안정적으로 진동이 덜한 상태로 움직이며 지역 최소값에서 쉽게 벗어날 수 있음\n",
        "\n",
        "<img src=\"https://i.ibb.co/sqYT2S9/momentum-2.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kfFIcnugVnL"
      },
      "source": [
        "**Momentum 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MRbOT6MgVnL",
        "outputId": "45f0e241-bd2e-4772-fb14-2ee8d29625e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameter before update: tensor([0., 0., 0., 0., 0.])\n",
            "parameter after update: tensor([ 0.0155, -0.0232, -0.0011,  0.0030,  0.0287])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "param = nn.Parameter(torch.zeros(5))\n",
        "momentum = torch.optim.SGD([param], lr=1e-2, momentum=0.9)\n",
        "\n",
        "prev_param = param.clone()\n",
        "param.grad = torch.randn_like(param.data)  # gradient accumulation\n",
        "momentum.step()  # parameter update\n",
        "\n",
        "print(f\"parameter before update: {prev_param.data}\")\n",
        "print(f\"parameter after update: {param.data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mjr8xuwz2Nd"
      },
      "source": [
        "#### **AdaGrad (Adaptive Gradient)**\n",
        "\n",
        "$ g_{t} =  g_{t-1} + \\frac{\\partial L}{\\partial \\theta_{t}} \\odot \\frac{\\partial L}{\\partial \\theta_{t}} \\\\\n",
        "\\theta_{t+1} = \\theta_{t} \\ - \\frac{\\mu}{\\sqrt{g_{t}} + \\epsilon} \\ \\frac{\\partial L}{\\partial \\theta_{t}} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$L$ : 손실함수 &#160;&#160; $\\theta$ : 모델의 파라미터 &#160;&#160; $\\mu$ : 학습률\n",
        "\n",
        "</span>\n",
        "</br>\n",
        "\n",
        "누적된 파라미터 기울기 제곱합을 이용하여 각 파라미터별로 다른 학습률을 가지고 업데이트가 진행되는 알고리즘으로,\n",
        "\n",
        "모델이 최적화되는 동안 각 파라미터별로 학습률이 계속해서 변하기 때문에 적응적 학습률이라고 부름 </br> </br>\n",
        "\n",
        "\n",
        "경사하강법을 통해 많이 이동한 파라미터는 적게 이동한 파라미터보다 최적화가 잘 이루어졌을 것이라고 가정하기 때문에\n",
        "\n",
        "적게 이동한 파라미터는 더 많이 움직이도록 학습률이 크게, 많이 이동한 파라미터는 적게 움직이도록 학습률이 작게 설정되는 특징을 가짐 </br> </br>\n",
        "\n",
        "\n",
        "<img src=\"https://classic.d2l.ai/_images/output_adagrad_2fb0ed_6_1.svg\" width=\"350\">\n",
        "\n",
        "하지만, 계속해서 과거의 파라미터 기울기를 제곱하여 더하기 때문에 학습률은 계속해서 감소하게 되는데,\n",
        "\n",
        "전역 최소값에 도달하기 전에 학습률이 0으로 수렴하여 더이상 파라미터 업데이트가 진행되지 않고 학습이 조기 종료될 수 있다는 단점이 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTBUF1jz6Z4"
      },
      "source": [
        "#### **RMSProp (Root Mean Square Propagation)**\n",
        "\n",
        "$ g_{t} = \\rho g_{t-1} + (1 - \\rho)\\ \\frac{\\partial L}{\\partial \\theta_{t}} \\odot \\frac{\\partial L}{\\partial \\theta_{t}} \\\\\n",
        "\\theta_{t+1} = \\theta_{t} - \\frac{\\mu}{\\sqrt{g_{t}} + \\epsilon} \\frac{\\partial L}{\\partial \\theta_{t}} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$L$ : 손실함수 &#160;&#160; $\\theta$ : 모델의 파라미터 &#160;&#160; $\\mu$ : 학습률 &#160;&#160; $\\rho$ : 지수 가중 평균의 평활 상수\n",
        "\n",
        "</span>\n",
        "</br>\n",
        "\n",
        "AdaGrad의 단점을 보완하기 위한 방법으로 등장한 알고리즘으로,\n",
        "\n",
        "단순히 파라미터 기울기의 제곱값을 누적하여 더하는 것이 아니라 지수 가중 평균을 사용함 </br> </br>\n",
        "\n",
        "***지수 가중 평균 (Exponentially Weighted Average, EWA)**\n",
        "\n",
        "$ v_{t} = \\beta v_{t-1} + (1-\\beta) \\theta_{t} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$v_{t-1}$ : 과거 데이터의 경향성 &#160;&#160; $\\theta_{t}$ : 현재 데이터 &#160;&#160; $\\beta$ : 지수 가중 평균의 평활 상수 (0~1)  \n",
        "\n",
        "</span>\n",
        "\n",
        "이동 평균을 구할 때 오래된 데이터가 미치는 영향을 지수적으로 감쇠하도록 만들어 주는 방법 </br> </br>\n",
        "\n",
        "<img src=\"https://d2l.ai/_images/output_rmsprop_251805_15_1.svg\" width=\"350\">\n",
        "\n",
        "지수 가중 평균으로 AdaGrad에 비해서는 비교적으로 최근에 이동한 정도를 많이 보며,\n",
        "\n",
        "최근에 적게 이동한 파라미터는 학습률을 크게 만들고, 최근에 많이 이동한 파라미터는 학습률을 작게 만드는 특징을 가지기 때문에\n",
        "\n",
        "전역 최소값에 도달하기 전에 학습률이 0으로 수렴하는 문제를 해결함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsqCTxu8z8zz"
      },
      "source": [
        "#### **Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "$ m_{t} = \\beta_{1} \\ m_{t-1} + (1 - \\beta_{1})\\ \\frac{\\partial L}{\\partial \\theta_{t}} \\\\\n",
        "v_{t} = \\beta_{2} \\ v_{t-1} + (1 - \\beta_{2}) \\frac{\\partial L}{\\partial \\theta_{t}} \\odot \\frac{\\partial L}{\\partial \\theta_{t}} \\\\\n",
        "\\hat{m_{t}} = \\frac{m_{t}}{1 - \\beta_{1}^{t}} \\quad \\hat{v_{t}} = \\frac{v_{t}}{1 - \\beta_{2}^{t}} \\\\\n",
        "\\theta_{t+1} = \\theta_{t} - \\frac{\\mu}{\\sqrt{\\hat{v_{t}}}+\\epsilon} \\hat{m_t} $\n",
        "\n",
        "<span style=\"font-size: 13px\">\n",
        "\n",
        "$L$ : 손실함수 &#160;&#160; $\\theta$ : 모델의 파라미터 &#160;&#160; $\\mu$ : 학습률 &#160;&#160; $\\beta_{1}$, $\\beta_{2}$ : 지수 가중 평균의 평활 상수\n",
        "\n",
        "</span>\n",
        "</br>\n",
        "\n",
        "Momentum과 RMSProp의 아이디어를 합친 최적화 알고리즘으로, 현재 가장 많이 사용되는 최적화 방법 중 하나\n",
        "\n",
        "파라미터 기울기의 제곱합과 모멘텀에 전부 지수 가중 평균이 적용되어, 각 파라미터별로 학습률과 이동량이 결정됨 </br> </br>\n",
        "\n",
        "<img src=\"https://i.ibb.co/6yz37Kc/adam-1.png\" width=\"350\">\n",
        "\n",
        "또한, 처음으로 파라미터 업데이트가 시작될 때는 파라미터 기울기의 제곱합과 모멘텀이 모두 0으로 값이 매우 낮기 때문에,\n",
        "\n",
        "업데이트 초반에 파라미터 기울기의 제곱합과 모멘텀 값을 보정해주는(Bias Correction) 수식이 있음 </br> </br>\n",
        "\n",
        "\n",
        "PyTorch에서는 AdaGrad, RMSProp, Adam과 같이 학습률이 적응적으로 변하는 옵티마이저의 경우,\n",
        "\n",
        "가중치 감쇠(Weight Decay)가 제대로 이루어지지 않아 SGD나 모멘텀 옵티마이저에 비해서 일반화 성능이 좋지 않은데,\n",
        "\n",
        "Adam에 가중치 감쇠가 잘 작동할 수 있도록 알고리즘을 살짝 변형한 AdamW가 있으며, 현재 많이 사용됨\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmdD_IqhgVnL"
      },
      "source": [
        "**AdamW 예시**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMAH-P8ugVnL",
        "outputId": "a4e5e687-b512-4d6b-bf0b-1b44a3a35cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parameter before update: tensor([0., 0., 0., 0., 0.])\n",
            "parameter after update: tensor([ 0.0010, -0.0010, -0.0010,  0.0010, -0.0010])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "param = nn.Parameter(torch.zeros(5))\n",
        "adamw = torch.optim.AdamW([param], lr=1e-3)\n",
        "\n",
        "prev_param = param.clone()\n",
        "param.grad = torch.randn_like(param.data)  # gradient accumulation\n",
        "adamw.step()  # parameter update\n",
        "\n",
        "print(f\"parameter before update: {prev_param.data}\")\n",
        "print(f\"parameter after update: {param.data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk_aOH4k0CtC"
      },
      "source": [
        "#### **옵티마이저에 따른 최적화 속도 비교**\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile25.uf.tistory.com%2Fimage%2F222B4F4F562BD0330EA41C\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1wMA7QtKf"
      },
      "source": [
        "이외에도 다양한 옵티마이저가 존재\n",
        "\n",
        "Pytorch : https://pytorch.org/docs/stable/optim.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}